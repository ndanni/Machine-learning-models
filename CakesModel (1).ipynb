{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "480074fe",
   "metadata": {},
   "source": [
    "Importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e97fcd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, applications\n",
    "import os\n",
    "import cv2\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator,img_to_array, load_img\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd336299",
   "metadata": {},
   "source": [
    "Mounting the dataset to the workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e16468d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "from azureml.core import Workspace, Dataset\n",
    "connect_str = os.getenv('DefaultEndpointsProtocol=https;AccountName=cakes;AccountKey=0BYHrONq+9pwMQafmGe+Oq4UqJ8lA2F41DZhw5lZSCRkIyddpxEh+bzk/msx88A70ihGFwrIcbcdINHUUmzskw==;EndpointSuffix=core.windows.net')\n",
    "connect_str\n",
    "\n",
    "url= Dataset.File.from_files('https://cakes.blob.core.windows.net/train')\n",
    "ws= Workspace.from_config()\n",
    "data_folder = os.path.join(os.getcwd(), 'data')\n",
    "os.makedirs(data_folder, exist_ok=True)\n",
    "\n",
    "url.download(data_folder, overwrite=True)\n",
    "\n",
    "cake_file_dataset = url.register(workspace=ws,\n",
    "                                name='train',\n",
    "                                description='training',\n",
    "                                create_new_version=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae43da6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Dataset\n",
    "connect_str = os.getenv('DefaultEndpointsProtocol=https;AccountName=cakes;AccountKey=0BYHrONq+9pwMQafmGe+Oq4UqJ8lA2F41DZhw5lZSCRkIyddpxEh+bzk/msx88A70ihGFwrIcbcdINHUUmzskw==;EndpointSuffix=core.windows.net')\n",
    "connect_str\n",
    "\n",
    "url= Dataset.File.from_files('https://cakes.blob.core.windows.net/test')\n",
    "\n",
    "data_folder2 = os.path.join(os.getcwd(), 'data2')\n",
    "os.makedirs(data_folder2, exist_ok=True)\n",
    "\n",
    "url.download(data_folder2, overwrite=True)\n",
    "\n",
    "cakess_file_dataset = url.register(workspace=ws,\n",
    "                                                 name='test2',\n",
    "                                                 description='testing',\n",
    "                                                 create_new_version=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a742bd8a",
   "metadata": {},
   "source": [
    "Labeling the data: the data is labeled based on the name of the folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee4e4a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 399 images belonging to 5 classes.\n",
      "Found 100 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "train = ImageDataGenerator(rescale=1/255)\n",
    "test = ImageDataGenerator(rescale=1/255)\n",
    "\n",
    "\n",
    "train_dataset = train.flow_from_directory('data/train',\n",
    "                                          target_size=(150,150),\n",
    "                                          batch_size = 32,\n",
    "                                          class_mode = 'sparse')\n",
    "                                         \n",
    "test_dataset = test.flow_from_directory('data2/test',\n",
    "                                          target_size=(150,150),\n",
    "                                          batch_size =32,\n",
    "                                          class_mode = 'sparse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "366f14a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BundtCake': 0,\n",
       " 'ChocolateCupCake': 1,\n",
       " 'OneStepCake': 2,\n",
       " 'TwoStepCake': 3,\n",
       " 'VanillaCupCake': 4}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f7686a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.Input(shape=(150, 150, 3)))\n",
    "model.add(layers.Conv2D(4, (3, 3), activation='relu'))\n",
    "model.add(layers.Dropout(rate=0.5))\n",
    "model.add(layers.MaxPooling2D((2, 2), strides=2))\n",
    "model.add(layers.Conv2D(4, (3, 3), activation='relu'))\n",
    "model.add(layers.Dropout(rate=0.5))\n",
    "model.add(layers.MaxPooling2D((2, 2), strides=2))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(10, activation= None ))\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Adam(),\n",
    "              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=[keras.metrics.SparseCategoricalAccuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9a39c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 148, 148, 4)       112       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 148, 148, 4)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 74, 74, 4)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 72, 72, 4)         148       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 72, 72, 4)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 36, 36, 4)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 5184)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                51850     \n",
      "=================================================================\n",
      "Total params: 52,110\n",
      "Trainable params: 52,110\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    " model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e56d8fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-8-c25dcb5c0ccb>:4: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 13 steps, validate for 4 steps\n",
      "Epoch 1/20\n",
      "13/13 [==============================] - 22s 2s/step - loss: 2.1565 - sparse_categorical_accuracy: 0.2180 - val_loss: 1.7660 - val_sparse_categorical_accuracy: 0.1800\n",
      "Epoch 2/20\n",
      "13/13 [==============================] - 19s 1s/step - loss: 1.5896 - sparse_categorical_accuracy: 0.2907 - val_loss: 1.9785 - val_sparse_categorical_accuracy: 0.2100\n",
      "Epoch 3/20\n",
      "13/13 [==============================] - 19s 1s/step - loss: 1.5256 - sparse_categorical_accuracy: 0.3534 - val_loss: 1.7626 - val_sparse_categorical_accuracy: 0.2900\n",
      "Epoch 4/20\n",
      "13/13 [==============================] - 19s 1s/step - loss: 1.4099 - sparse_categorical_accuracy: 0.4135 - val_loss: 1.6286 - val_sparse_categorical_accuracy: 0.3200\n",
      "Epoch 5/20\n",
      "13/13 [==============================] - 19s 1s/step - loss: 1.2714 - sparse_categorical_accuracy: 0.4912 - val_loss: 1.5460 - val_sparse_categorical_accuracy: 0.3300\n",
      "Epoch 6/20\n",
      "13/13 [==============================] - 22s 2s/step - loss: 1.1802 - sparse_categorical_accuracy: 0.5313 - val_loss: 1.4891 - val_sparse_categorical_accuracy: 0.4400\n",
      "Epoch 7/20\n",
      "13/13 [==============================] - 22s 2s/step - loss: 1.0820 - sparse_categorical_accuracy: 0.5789 - val_loss: 1.4626 - val_sparse_categorical_accuracy: 0.4600\n",
      "Epoch 8/20\n",
      "13/13 [==============================] - 23s 2s/step - loss: 0.9644 - sparse_categorical_accuracy: 0.6792 - val_loss: 1.4298 - val_sparse_categorical_accuracy: 0.5000\n",
      "Epoch 9/20\n",
      "13/13 [==============================] - 22s 2s/step - loss: 0.8688 - sparse_categorical_accuracy: 0.6967 - val_loss: 1.3864 - val_sparse_categorical_accuracy: 0.5000\n",
      "Epoch 10/20\n",
      "13/13 [==============================] - 21s 2s/step - loss: 0.8087 - sparse_categorical_accuracy: 0.7293 - val_loss: 1.3523 - val_sparse_categorical_accuracy: 0.5000\n",
      "Epoch 11/20\n",
      "13/13 [==============================] - 23s 2s/step - loss: 0.7147 - sparse_categorical_accuracy: 0.7644 - val_loss: 1.3197 - val_sparse_categorical_accuracy: 0.4700\n",
      "Epoch 12/20\n",
      "13/13 [==============================] - 20s 2s/step - loss: 0.7016 - sparse_categorical_accuracy: 0.7544 - val_loss: 1.3307 - val_sparse_categorical_accuracy: 0.4600\n",
      "Epoch 13/20\n",
      "13/13 [==============================] - 19s 1s/step - loss: 0.6001 - sparse_categorical_accuracy: 0.7970 - val_loss: 1.3106 - val_sparse_categorical_accuracy: 0.4700\n",
      "Epoch 14/20\n",
      "13/13 [==============================] - 21s 2s/step - loss: 0.5487 - sparse_categorical_accuracy: 0.8471 - val_loss: 1.3027 - val_sparse_categorical_accuracy: 0.4800\n",
      "Epoch 15/20\n",
      "13/13 [==============================] - 19s 1s/step - loss: 0.4478 - sparse_categorical_accuracy: 0.8697 - val_loss: 1.2710 - val_sparse_categorical_accuracy: 0.4800\n",
      "Epoch 16/20\n",
      "13/13 [==============================] - 19s 1s/step - loss: 0.3988 - sparse_categorical_accuracy: 0.8922 - val_loss: 1.2694 - val_sparse_categorical_accuracy: 0.5100\n",
      "Epoch 17/20\n",
      "13/13 [==============================] - 19s 1s/step - loss: 0.3839 - sparse_categorical_accuracy: 0.8947 - val_loss: 1.2427 - val_sparse_categorical_accuracy: 0.5200\n",
      "Epoch 18/20\n",
      "13/13 [==============================] - 20s 2s/step - loss: 0.3755 - sparse_categorical_accuracy: 0.8872 - val_loss: 1.2421 - val_sparse_categorical_accuracy: 0.4900\n",
      "Epoch 19/20\n",
      "13/13 [==============================] - 20s 2s/step - loss: 0.3379 - sparse_categorical_accuracy: 0.9073 - val_loss: 1.2443 - val_sparse_categorical_accuracy: 0.5200\n",
      "Epoch 20/20\n",
      "13/13 [==============================] - 20s 2s/step - loss: 0.2892 - sparse_categorical_accuracy: 0.9273 - val_loss: 1.2580 - val_sparse_categorical_accuracy: 0.5300\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9cbb24def0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_dataset,\n",
    "        #steps_per_epoch = 5,\n",
    "         epochs = 20,\n",
    "         validation_data = test_dataset\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8051c316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate on test data\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "13/13 [==============================] - 16s 1s/step - loss: 0.6696 - sparse_categorical_accuracy: 0.9273\n",
      "test loss, test acc: [0.6695881967361157, 0.9273183]\n",
      "Generate predictions for 3 samples\n",
      "predictions shape: (32, 10)\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test data using `evaluate`\n",
    "print(\"Evaluate on test data\")\n",
    "results = model.evaluate(train_dataset)\n",
    "print(\"test loss, test acc:\", results)\n",
    "\n",
    "# Generate predictions (probabilities -- the output of the last layer)\n",
    "# on new data using `predict`\n",
    "print(\"Generate predictions for 3 samples\")\n",
    "predictions = model.predict(test_dataset[1])\n",
    "print(\"predictions shape:\", predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65075792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading and preprocessing image...\n",
      "Image ID: 3, Label: TwoStepCake\n"
     ]
    }
   ],
   "source": [
    "image_path = 'data2/test/BundtCake/100.jpg'  \n",
    "img_width, img_height = 150, 150\n",
    "\n",
    "# orig = cv2.imread(image_path)  \n",
    "\n",
    "print(\"[INFO] loading and preprocessing image...\")  \n",
    "image = load_img(image_path, target_size=(img_width, img_height))  \n",
    "image2 = img_to_array(image)\n",
    "\n",
    "# important! otherwise the predictions will be '0'  \n",
    "image3 = image2 / 255  \n",
    "\n",
    "image4 = np.expand_dims(image2, axis=0)\n",
    "\n",
    "# build the VGG16 network  \n",
    "model = applications.VGG16(include_top=False, weights='imagenet')  \n",
    "\n",
    "# get the bottleneck prediction from the pre-trained VGG16 model  \n",
    "bottleneck_prediction = model.predict(image4)\n",
    "\n",
    "generator_top = test_dataset\n",
    "\n",
    "n_classes = len(generator_top.class_indices)\n",
    "\n",
    "# build top model  \n",
    "model = keras.Sequential() \n",
    "model.add(layers.Flatten(input_shape=bottleneck_prediction.shape[1:]))  \n",
    "model.add(layers.Dense(256, activation='relu'))  \n",
    "model.add(layers.Dropout(0.5))  \n",
    "model.add(layers.Dense(n_classes, activation=None))  \n",
    "\n",
    "# model.load_weights(top_model_weights_path)  \n",
    "\n",
    "# use the bottleneck prediction on the top model to get the final classification  \n",
    "class_predicted = model.predict_classes(bottleneck_prediction) \n",
    "\n",
    "inID = class_predicted[0]  \n",
    "\n",
    "class_dictionary = generator_top.class_indices  \n",
    "\n",
    "inv_map = {v: k for k, v in class_dictionary.items()}  \n",
    "\n",
    "label = inv_map[inID]  \n",
    "\n",
    "# get the prediction label  \n",
    "print(\"Image ID: {}, Label: {}\".format(inID, label)) "
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
